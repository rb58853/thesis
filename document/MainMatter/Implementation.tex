\chapter{Detalles de Implementación y Experimentos}\label{chapter:implementation}


\section{Detalles de implementaci\'on}
La sección de detalles de implementación proporcionar\'a una explicación detallada sobre la arquitectura del proyecto. Esta explicación cubrir\'a aspectos como la escalabilidad del sistema y los detalles específicos de implementación que son relevantes para el proyecto.

\subsection{Estructura}
El proyecto se estructura en distintas secciones. Cada una de estas tiene un propósito definido y se explica a continuación, destacando su uso, funciones y objetivos.

\subsubsection{Modelos descriptivos}
 En la sección de modelos de descripción, se realiza la implementación de cada uno de los modelos integrados. Aquí también se pueden agregar los modelos futuros que se deseen incorporar. Actualmente, los modelos BLIP[\cite{huggingface2022blip}] y BLIP2[\cite{huggingface-blip2}] est\'an integrados, cada uno de ellos carga su contenido desde la biblioteca \textit{Transformers de Hugging Face}. La estructura común para cargar modelos desde esta biblioteca se muestra a continuación, tomando como ejemplo el modelo BLIP[\cite{huggingface2022blip}].

\begin{lstlisting} [language=Python]
from transformers import BlipProcessor, BlipForConditionalGeneration
BLIP.PROCESSOR = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
BLIP.MODEL = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to("cuda")
\end{lstlisting}

La función \verb|from_pretrained()| de la biblioteca \textit{Transformers} permite cargar un modelo preentrenado desde Hugging Face. Esta función puede cargar el modelo a partir de una identificación de modelo, un directorio local donde se encuentran los pesos del modelo guardados, o un archivo de estado \textit{PyTorch}. 

\subsubsection{Modelos de segmentaci\'on}
De manera similar a la sección de modelos descriptivos, existe una sección dedicada a los modelos de segmentación. En esta sección, se integra el modelo SAM[\cite{huggingface2022sam}], que se utiliza en el código de tal manera que permita manipular las salidas y par\'ametros del modelo sin necesidad de alterar el código. Para lograr esto, se dispone de una clase \textit{SAM-Environment} en la sección de entorno, la cual contiene todos los par\'ametros modificables de este modelo. De esta forma, si se desea cambiar su comportamiento, se tiene la opción de hacerlo.

La codificación de SAM[\cite{huggingface2022sam}] permite solicitar a las segmentaciones im\'agenes de tipo "box" o de tipo "mask". El primer tipo devuelve todos los píxeles encontrados en el cuadro de segmentación, mientras que el segundo devuelve los píxeles de la m\'ascara de segmentación con el fondo blanco. La elección de qué tipo de segmentación utilizar queda a discreción del procesador de im\'agenes.

\subsubsection{Generadores de \textit{embeddings}}
En esta sección se integran los modelos de generación de \textit{embeddings}. En el caso particular de este trabajo, se ha integrado el modelo CLIP[\cite{git-clip}]. Al igual que en los casos anteriores, si se necesita integrar otros modelos de generación de \textit{embeddings}, debe hacerse dentro de esta carpeta.

\subsubsection{Entorno(\textit{Environment})}
En esta sección se concentra todo lo relacionado con el entorno. Desde aquí, se pueden realizar modificaciones en todos los par\'ametros del sistema que se desee. Por ejemplo, si se requiere que el código utilice descripciones en el c\'alculo de similitudes, se debe ajustar la variable correspondiente en la clase mencionada. En el presente ejemplo, se debe cambiar la variable \textit{USE-CAPTION-MODEL = False} a \textit{True} en la clase \textit{ImageEnvironment}. Adem\'as, se proporciona un ejemplo de una clase en el entorno, ya que este es el código potencialmente modificable.

\begin{lstlisting}[language=Python]
class ImageEnvironment:
    ### Variables 
    - `USE_CAPION_MODEL` Indica que se usar\'a un modelo de descripciones de im\'agenes
    - `CAPTION_MODEL` Este es el modelo que se usar\'a para describir la im\'agen
    - `CAPTION_IMPORTANCE` Importancia de la descripci\'on, define que tan importante ser la descripci\'on de una imagen para la similitud. Valor=1 indica que tiene igual importancia que la imagen.
    - `KEY_IMAGES`: Esto es el tipo de imagen que usar\'a como principal. (box or mask).
    ### Funciones
    - `max_similarity()`: Devuelve la m\'axima similitud que se puede alcanzar en la distancia entre dos im\'agenes. 
    '''

    USE_CAPION_MODEL = False
    CAPTION_MODEL =BLIP
    CAPTION_IMPORTANCE = 1
    KEY_IMAGES = 'box'

    def max_similarity():
        return ImageEmbeddingEnv.MAX_DISTANCE/ImageEmbeddingEnv.POS_UMBRAL

\end{lstlisting}

\subsubsection{Caracter\'isticas(\textit{features})}
Esta es la sección m\'as crucial, donde se implementan las clases que abordan los conceptos fundamentales del trabajo: Image, ImageSet, Text, TextSet. Idealmente, esta clase no debería ser modificada, a menos que se desee asignar nuevas funcionalidades a estas clases. Cada clase tiene sus propios procesos internos, pero su principal objetivo es almacenar características en una clase determinada. 
La clase \textit{image } posee una subclase que se encarga exclusivamente de crear funciones para el c\'alculo de similitudes de vecindad. El resto de los componentes de ambas clases dependen únicamente de procesos externos, no implementados dentro de la clase en sí. Por ejemplo, el proceso de segmentación de im\'agenes y el an\'alisis sint\'actico del texto se llevan a cabo en módulos específicos encargados de esta función en particular.

\subsubsection{Gram\'atica}
En esta sección se encuentran los tokenizadores y analizadores sint\'acticos que procesan las consultas en forma de texto. Cada uno de estos est\'a diseñado para ser escalable, lo que permite un futuro mejor procesamiento y comprensión del lenguaje.

\subsubsection{Recuperaci\'on de informaci\'on}
En esta sección se ubica el motor de recuperación de información y la base de datos. Ambas son clases que implementan el comportamiento necesario para recuperar y guardar la información. El sistema de recuperación tiene dos funciones principales: una función de ranking y una función de búsqueda. La primera se enfoca en calcular la similitud entre una consulta y todas las im\'agenes de la base de datos utilizando la distancia del coseno. Se establece un umbral de tamaño m\'aximo del ranking y se guardan las n mejores im\'agenes. Luego, las im\'agenes en ese ranking son comparadas con la consulta utilizando el algoritmo principal propuesto. Este proceso se divide en dos partes, ya que calcular la similitud del coseno es considerablemente m\'as r\'apido que el c\'alculo utilizando el algoritmo principal. Para calcular la similitud de 10,000 im\'agenes, el algoritmo principal tarda, aproximadamente, 42 segundos usando una GPU v100, mientras que la similitud por coseno tarda ese tiempo para 1,000,000 de im\'agenes.

La clase data tiene como principal objetivo acumular la información, guardarla en disco y cargarla desde el disco. Para ello cuenta con funciones para guardar y cargar desde una dirección, así como funciones para agregarle nuevos datos desde el código y, teniendo estos, poder guardarlos. La información guardada y cargada por la base de datos ser\'a, de cada imagen, el embedding, la posición, el nombre, referencia a los vecinos y similitud de cercanía a estos, y límites de una imagen.

\subsubsection{Similitud}
En esta sección se implementan todas las funciones de similitud que se utilizar\'an para el c\'alculo. Si se desea agregar una nueva función de similitud, debe hacerse en esta clase.

\subsubsection{Main} 
Finalmente, se llega a la función global, desde la cual se utilizan los dem\'as módulos. Esta sección est\'a contenida por un archivo \textit{.ipynb} desde el cual se llama y ejecuta cada función deseada. El uso de un \textit{notebook} es debido a la necesidad de ejecutar el proyecto en una m\'aquina virtual de Google Colab.


\section{Experimentos}
El objetivo principal de la propuesta presentada es mejorar la similitud entre textos m\'as descriptivos y las im\'agenes correspondientes que contienen la mayor cantidad de detalles descritos en el texto. Para lograr esto, se han implementado tres variantes distintas que se centran en recuperar im\'agenes para descripciones posicionales en la consulta.

En una fase experimental, cada una de estas implementaciones ha sido probada y comparada tanto entre los resultados que se obtiene de cada fase, como entre cada resultado y la recuperación simple de consulta e imagen, similitud por distancia del coseno. Este proceso permite evaluar la efectividad de cada método y determinar cu\'al ofrece los mejores resultados.

Para facilitar la comprensión de los resultados y los experimentos realizados, se proporciona un conjunto de im\'agenes de muestra. Estas im\'agenes han pasado por el sistema de recuperación, donde se han aplicado medidas de similitud y se ha utilizado cada variante. De esta forma, se puede analizar cómo cada método afecta a la recuperación de las im\'agenes y su grado de similitud con los textos descriptivos.


\begin{figure}
\centering
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_1.jpg}
 \caption{ }
 \label{fig:1}
\end{minipage}%
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_2.jpg}
 \caption{ }
 \label{fig:2}
\end{minipage}
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_3.jpg}
 \caption{ }
 \label{fig:3}
\end{minipage}
\end{figure}

\begin{figure}
\centering
\begin{minipage}{0.3\textwidth}%
 \includegraphics[width=\textwidth]{Graphics/Images/image_4.jpg}
 \caption{ }
 \label{fig:4}
\end{minipage}%
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_5.jpg}
 \caption{ }
 \label{fig:5}
\end{minipage}
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_6.jpg}
 \caption{ }
 \label{fig:6}
\end{minipage}
\end{figure}

\begin{figure}
\centering
\begin{minipage}{0.3\textwidth}%
 \includegraphics[width=\textwidth]{Graphics/Images/image_7.jpg}
 \caption{ }
 \label{fig:7}
\end{minipage}%
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_8.jpg}
 \caption{ }
 \label{fig:8}
\end{minipage}
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_9.jpg}
 \caption{ }
 \label{fig:9}
\end{minipage}
\end{figure}

\begin{figure}
\centering
\begin{minipage}{0.3\textwidth}%
 \includegraphics[width=\textwidth]{Graphics/Images/image_10.jpg}
 \caption{ }
 \label{fig:10}
\end{minipage}%
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_11.jpg}
 \caption{ }
 \label{fig:11}
\end{minipage}
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_12.jpg}
 \caption{ }
 \label{fig:12}
\end{minipage}
\end{figure}

\begin{figure}
\centering
\begin{minipage}{0.3\textwidth}%
 \includegraphics[width=\textwidth]{Graphics/Images/image_13.jpg}
 \caption{ }
 \label{fig:13}
\end{minipage}%
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_14.jpg}
 \caption{ }
 \label{fig:14}
\end{minipage}
\begin{minipage}{0.3\textwidth}
 \includegraphics[width=\textwidth]{Graphics/Images/image_15.jpg}
 \caption{ }
 \label{fig:15}
\end{minipage}
\end{figure}

\subsection{Procesamiento de im\'agenes usando \textit{embeddings}}
El uso de esta variante demostró ser particularmente efectivo para la recuperación de consultas detalladas en términos de posición. Para ilustrar cómo funciona, se muestra un ejemplo que destaca una mejora significativa en la recuperación de información precisa.

Dos consultas fueron formuladas inicialmente, las cuales parecen bastante similares. Sin embargo, la situación posicional es el factor principal que las diferencia. En el primer escenario, la consulta no coincide con las posiciones de los objetos, lo que resulta en un valor de relevancia mucho menor que la consulta que hace referencia precisa a las relaciones posicionales entre los conceptos de la imagen. En este caso, la imagen es bastante clara, lo que facilita que el modelo CLIP[\cite{clip}] no genere errores y lance resultados no esperados. Es bastante claro lo que es cada objeto en la imagen.

Este ejemplo demuestra cómo la variante implementada puede mejorar la precisión de la recuperación de información, especialmente cuando se trata de consultas detalladas en términos de posición

\begin{verbatim}
"a cat on left side of dog":
value: 0.3123254179954529

"there is a dog on left of a cat":
value:0.6470594258080886
\end{verbatim}

\begin{figure}[H]
\centering
 \includegraphics[width=0.5\textwidth]{Graphics/dog_in_left.png}
 \caption{ }
 \label{fig:dog}
\end{figure}

Paralelamente, se determinó la relevancia para las mismas consultas e im\'agenes, utilizando únicamente el mismo procedimiento que emplea CLIP[\cite{clip}], similitud del coseno[\cite{git-clip}], para encontrar similitud. En este caso, se calculó la similitud mediante la distancia del coseno.

\begin{verbatim}
"a cat on left side of dog":
value: 0.3123254179954529

"there is a dog on left of a cat":
value: 0.31687411665916443
\end{verbatim}

Es posible observar que la relevancia de la imagen para ambas consultas es muy similar. Sin embargo, el resultado de esta medida de similitud se inclina hacia la consulta que expresaba incorrectamente las relaciones posicionales.

Para una mejor observación de este proceso, se ejecutó la recuperación de información en la muestra de im\'agenes. De esta manera, se pueden observar los resultados alcanzados por esta metodología. A continuación, se muestra una serie de consultas y resultados de aplicar la misma.

\begin{verbatim}
"a cat on left of an another cat":
image_1: 0.5910724142355481
image_2: 0.552273309297955
image_3: 0.57958666555157
image_4: 0.20404787361621857
image_5: 0.17712320387363434
image_6: 0.27128875255584717
image_7: 0.16639965772628784
image_8: 0.15555395185947418
image_9: 0.5865751181561818
image_10: 0.6205110420687305
image_11: 0.2572214603424072
image_12: 0.27217862010002136
image_13: 0.27809980511665344
image_14: 0.569566040832626
image_15: 0.6235474817208575

"a cat on left side of dog":
image_1: 0.24559526145458221
image_2: 0.5900955027099477
image_3: 0.5933583085861937
image_4: 0.2507782280445099
image_5: 0.16662538051605225
image_6: 0.24885490536689758
image_7: 0.28234352743521146
image_8: 0.14847618341445923
image_9: 0.6239545388814016
image_10: 0.3215339481830597
image_11: 0.2331039309501648
image_12: 0.24892988801002502
image_13: 0.6274699419732972
image_14: 0.3123254179954529
image_15: 0.27217918634414673

"there is a dog on left of a cat":
image_1: 0.2507342994213104
image_2: 0.5737969104184191
image_3: 0.6017459795652185
image_4: 0.25493425130844116
image_5: 0.17491364479064941
image_6: 0.26476845145225525
image_7: 0.17373675107955933
image_8: 0.16383123397827148
image_9: 0.30724239349365234
image_10: 0.6605514306535915
image_11: 0.2553481161594391
image_12: 0.2543933391571045
image_13: 0.30912819504737854
image_14: 0.6470594258080886
image_15: 0.2859894335269928

"a woman beside to cat, a woman in right of a dog":
image_1: 0.25769439339637756
image_2: 0.251406192779541
image_3: 0.27823591232299805
image_4: 0.2350061535835266
image_5: 0.1671127825975418
image_6: 0.23403123021125793
image_7: 0.17912636697292328
image_8: 0.16113649308681488
image_9: 0.3029889166355133
image_10: 0.3197519779205322
image_11: 0.2800091803073883
image_12: 0.23648248612880707
image_13: 0.29852136969566345
image_14: 0.3131987154483795
image_15: 0.25541138648986816

\end{verbatim}

En estos ejemplos, se pueden apreciar buenos resultados relacionados con las posiciones espaciales. Un claro ejemplo de ello es la consulta "there are a dog in left of a cat". Los resultados se ajustan a las relaciones espaciales y no encuentran relevancia alta para im\'agenes donde cada uno de sus conceptos coincide en gran medida con lo pasado por la consulta. Por ejemplo, las im\'agenes de las figuras \ref{fig:13} y \ref{fig:9} poseen conceptos muy parecidos, pero posiciones diferentes.

Por otro lado, se aprecian resultados con alta precisión que, para una observación humana, no deberían tenerla. El caso de la imagen de la figura \ref{fig:3} resulta relevante dado que CLIP[\cite{clip}] entiende que el objeto a la izquierda tiene gran parecido con "dog", y por lo tanto, el sistema lo encuentra relevante. En el caso de la imagen \ref{fig:2}, las posiciones son ambiguas, los objetos est\'an superpuestos y el modelo de segmentación, SAM[\cite{sam-paper}], genera particiones varias para el perro y para el gato, lo que suele causar una ambigüedad posicional a nivel de im\'agenes.

Paralelamente a este ejemplo, se aprecia que la consulta " a cat on left of a dog", que utiliza los mismos conceptos, también devuelve resultados satisfactorios en cuanto a la posición de los mismos.

El resultado de la última consulta indica que no se encontraron, para ninguna imagen, relaciones posicionales planteadas en la consulta. Esto es debido a que la imagen de la mujer resulta poco relevante para el texto "woman", por lo tanto, no se entra a buscar vecindades de este concepto, y nunca se encuentra relevancia con lo pasado por la consulta


\begin{figure}[H]
\centering
 \includegraphics[width=0.5\textwidth]{Graphics/woman.png}
 \caption{ }
 \label{fig:woman}
\end{figure}

\begin{verbatim}
"woman"
value: 0.20488907396793365
\end{verbatim}

Como se mencionó en el \textit{capítulo 2}, el algoritmo de similitud est\'a diseñado para que, a mayor descripción de la imagen, sean m\'as precisos los resultados. Como resultado de esto, se muestran algunas consultas muy detalladas en cuanto a posición, con el objetivo de recuperar im\'agenes de forma m\'as precisa. En este caso, a una consulta detallada solo mejora los resultados si encuentra las descripciones específicas pasadas.

Por ejemplo, para la consulta \textit{in the center of image there are tow dogs sitting in a black and white couch. A dog beside to a dark gray dog.  below the dog there are a black couch. right to a couch there are a black statue of buda. On top of couch are a white window.}, se obtienen los siguientes resultados.
\begin{verbatim}
image_1: 0.19233258068561554
image_2: 0.27561962241802707
image_3: 0.8454822663938976
image_4: 1.6529658667220515
image_5: 0.18111929297447205
image_6: 0.17510709166526794
image_7: 0.19716928899288177
image_8: 0.2104945033788681
image_9: 0.5446422211089514
image_10: 0.560958578905552
image_11: 0.2174830138683319
image_12: 0.21318000555038452
image_13: 0.24322502315044403
image_14: 0.5096454837731963
image_15: 0.23101291060447693
\end{verbatim}
Se observa que la imagen recuperada,, figura \ref{fig:dogs_couch}, posee una similitud muy alta, la misma, para ojos humanos, es bastante cercana a lo descrito en la consulta de entrada. La siguiente imagen cercana en similitud también se acerca a lo descrito en la consulta.

Estos resultados refuerzan la eficacia del algoritmo de similitud diseñado para mejorar la precisión de los resultados de recuperación de im\'agenes. Al proporcionar una descripción detallada de la imagen, el algoritmo puede identificar mejor las características clave y las relaciones espaciales, lo que resulta en una recuperación de im\'agenes m\'as precisa.

\begin{figure}[H]
\centering
 \includegraphics[height=80mm]{Graphics/Images/image_4.jpg}
 \caption{ }
 \label{fig:dogs_couch}
\end{figure}

\subsection{Procesamiento de im\'agenes usando descripciones}
En esta sección, se presentan los resultados de la variante que utiliza descripciones, y se analiza la ineficiencia de la misma. Como resultado de esto, el uso de descripciones sin im\'agenes suele ser igualmente menos eficiente.

Esta variante presenta un problema fundamental: los textos de concepto cercanos suelen ser muy similares. Por ejemplo, en las muestras e im\'agenes, para las im\'agenes y consultas relacionadas a perros y gatos, las relevancias alcanzadas son muy similares, dado que en este caso los conceptos \textbf{dog} y \textbf{cat} tienen una relación de similitud de $0.91283133$. Esto implica que las im\'agenes que tengan \textbf{dog} como descripción ser\'an altamente relevantes con los textos que tengan \textbf{cat} en su contenido, y en estos casos son muy similares. A continuaci\'on se muestran tres de las consultas con sus respectivas similitudes hacia las im\'agenes.

\begin{verbatim}

"a cat on left side of dog":
image_1: 1.1524357352905104
image_2: 1.3433827930426756
image_3: 1.2726673896730163
image_4: 1.2426372272442288
image_5: 0.6868996450696936
image_6: 0.6899906356873706
image_7: 0.6601000538778263
image 8: 0.5863789420884641
image_9: 1.3568643812126333
image_10: 1.3029270330526308
image_11: 1.228132925709438
image_12: 1.1415868365589665
image_13: 1.2949859036677203
image_14: 1.309315788897958
image_15: 1.269946229080809

"there is a dog on left of a cat:
image_1: 1.2083463593593047
image_2: 1.3538045434242947
image_3: 1.244656483332271
image_4: 1.25616415288856
image_5: 0.6519800801549985
image_6: 0.7511959871556898
image_7: 0.6586201567797191
image 8: 0.5911412861692614
image_9: 1.32958947197954
image_10: 1.3701337275645442
image_11: 1.3088403469722087
image_12: 1.2220715210928654
image_13: 1.2619162228862222
image_14: 1.4098256637597801
image_15: 1.3314236030203248

"vase of roses in top of a bedside table":
image_1: 0.4997751770591399
image_2: 0.5835944445734702
image_3: 0.5381005093464214
image_4: 0.5806767925529962
image_5: 0.5807832418280381
image_6: 0.530520117014973
image_7: 1.4137167510662856
image 8: 0.45439535468733877
image_9: 0.5458514239447663
image_10: 0.5084185778157145
image_11: 0.57788808475214
image_12: 0.5664107286655431
image_13: 0.5993313035010336
image_14: 0.5535414798660193
image_15: 0.5891194603734591

\end{verbatim}

En la consulta \textit{" a cat on left side of dog"}, se esperaría que la imagen de la figura \ref{fig:13} sea m\'as relevante que la imagen de la figura \ref{fig:14}. Sin embargo, en este caso no se obtuvieron buenos resultados. Para lograr mejores resultados, se pueden probar m\'as variaciones de la importancia del texto y el uso de otros modelos descriptivos.

Por otro lado, cuando se trabaja trabajando con conceptos distintos como rosas y mesas, la diferencia de similitud es notable en cuanto a los resultados recuperados.

\subsection{C\'alculos de posici\'on}
En esta sección, se pretende mostrar algunos ejemplos de cómo el uso de las posiciones en los conceptos impacta en el resultado de la similitud. Para ello, se observar\'a la imagen \textit{Image 10} del \textit{ImageSet} en la figura \ref{fig:roses}, a la que se le pasan textos en cada caso. Se ver\'a cómo se comportan los cambios de vecinos y de posiciones usados.

Para comprender los ejemplos próximamente presentados, se explica la leyenda que usan los mismos.

\begin{itemize}
\item \verb|TEXT: "texto"| indica que "texto" es el texto principal del \textit{Text}.
\item \verb|POS: (x,y)| indica la posici\'on en la cual se encuentra la \textit{Image} analizada.
\item \verb|label: ["neigbords"]| este label est\'a dado por cada una de las etiquetas de los vecinos de un \textit{Text}, solo mostrando las que al menos, poseen un vecino; cada elemento que corresponde a esta corresponde a un vecino.
\item \verb|SIMILARTY| indica que lo pr\'oximo a mostrar ser\'an similitudes.
\item \verb|cosine: value| indica el valor de la similitud del coseno.
\item \verb|cosine_pos: value| indica el valor de la similitud del coseno teniendo en cuenta la posici\'on de la imagen.
\item \verb|using region: value| indica el valor de similitud por posici\'on y coseno, adem\'as, teniendo en cuenta los vecinos de cada concepto.
\end{itemize}

\begin{figure}[H]
\centering
 \includegraphics[height=80mm]{Graphics/vase_roses.jpg}
 \caption{ }
 \label{fig:roses}
\end{figure}

\begin{verbatim}
TEXT: A bouquet of roses
POS: (0.5, 0.5)
s:[a table]
SIMILARITY
    cosine: 0.2806638479232788
    cosine_pos: 0.31585035808965767
    using region relation: 0.410122760
——————————————————————————————————————————

TEXT: a roses
POS: (0.5, 0.5)
s:[a bedside table]
SIMILARITY
    cosine: 0.26832765340805054
    cosine_pos: 0.301967588
    using region relation: 0.3985802
——————————————————————————————————————————

TEXT: A bouquet of roses
POS: (0.5, 0.5)
s:[a gray and black bedside table]
SIMILARITY
    cosine: 0.2806638479232788
    cosine_pos: 0.3158503580896
    using region relation: 0.4276975
——————————————————————————————————————————

TEXT: A bouquet of roses
POS: (0.5, 0.5)
s:[a gray and black bedside table, a white drawer]
SIMILARITY
    cosine: 0.2806638479232788
    cosine_pos: 0.31585035808965767
    using region relation: 0.532067
\end{verbatim}
El uso de posiciones globales puede mejorar el resultado de la similitud, en estos casos se observa cómo mejora la relevancia de la imagen si la posición coincide con la dada. Por otro lado, el uso de regiones, o de vecindades posicionales, puede tanto mejorar los resultados de la similitud como empeorarlos. En este ejemplo se aprecia entonces la influencia de las posiciones para el resultado del c\'alculo de similitud.

A continuaci\'on se muestra un ejemplo, usando la misma leyenda, de un caso donde al no detectar ningún vecino que cumpla con lo esperado en una posición, la similitud por regiones disminuye entonces su valor. La imagen evaluada es la segmentación 17 de la figura \ref{fig:dogs_pillow}.

\begin{figure}[H]
\centering
 \includegraphics[height=85mm]{Graphics/dog_pillow.jpg}
 \caption{ }
 \label{fig:dogs_pillow}
\end{figure}
\begin{verbatim}
TEXT: a dog laying on a blue pillow
POS:(0.5, 0.5)
w:[a apple]
SIMILARITY
    cosine: 0.25085294246673584
    cosine_pos: 0.28454447699
    using region relation: 0.2739276
\end{verbatim}


\subsection{Observaciones}
Como desarrollo principal, se buscaba lograr recuperar im\'agenes a partir de una consulta en lenguaje natural. Sin embargo, se observó que la metodología es aplicable para recuperar im\'agenes utilizando otras im\'agenes como consulta. En este caso, se toma en cuenta las relaciones posicionales de cada objeto en cuestión junto con sus conceptos agrupados en \textit{embeddings}. El resultado de la recuperación de im\'agenes debe ser ajustado para lograr una recuperación precisa. Una vez se ajusten los par\'ametros para la recuperación imagen-imagen, ser\'a posible realizar dicha tarea. N\'otese que este sistema de recuperaci\'on imagen-imagen basado en posiciones y conceptos, constituye un enfoque distinto al utilizado, por lo general, en la b\'usqueda de im\'agenes con im\'agenes como consulta.


