\chapter*{Introducci\'on}\label{chapter:introduction}
\addcontentsline{toc}{chapter}{Introducci\'on}

En el pasado, el uso de la inteligencia artificial estaba restringido y se empleaba principalmente en casos de uso espec\'ificos. Las entidades que la utilizaban sol\'ian estar familiarizadas con este campo y ten\'ian objetivos bien definidos.

En la actualidad se ha logrado un avanze considerable en este campo, obteniendo resultados que hace años parec\'ian poco probables. Cada vez m\'as personas est\'an comenzando a aprovechar estos beneficios, y la tecnolog\'ia est\'a cambiando r\'apidamente, con la inteligencia artificial siendo el centro de todo. Si bien antes esta tecnolog\'ia era menos utilizada,  el lanzamiento de nuevos modelos de lenguaje accesibles para todos, como GPT, ha despertado el inter\'es y la adopci\'on de la inteligencia artificial por parte de un p\'ublico m\'as amplio.

Es innegable que la interacci\'on entre los seres humanos y las m\'aquinas est\'a experimentando cambios significativos. Cada vez se les encomiendan m\'as tareas que antes eran exclusivas de las personas, como la traducci\'on, el diseño de im\'agenes e incluso la generaci\'on de c\'odigo, que ahora son abordadas por la inteligencia artificial, al menos hasta cierto grado de correctitud.

Como parte este avance, el campo de la visi\'on artificial tambi\'en ha evolucionado notablemente. La visi\'on artificial permite a las computadoras y sistemas extraer informaci\'on relevante de im\'agenes digitales, videos y otras entradas visuales. Gracias a esta capacidad, dichos sistemas pueden tomar medidas o realizar recomendaciones basadas en dicha informaci\'on. Podr\'iamos decir que si la inteligencia artificial permite a las computadoras pensar, la visi\'on artificial les permite ver, observar y comprender.

El impresionante progreso del aprendizaje autom\'atico en los \'ultimos años, especialmente el aprendizaje profundo (Deep Learning), ha revolucionado el campo de la visi\'on artificial, posibilitando nuevas aplicaciones que antes parec\'ian inimaginables. Desde diagn\'osticos de im\'agenes en el campo de la medicina, la automatizaci\'on de veh\'iculos, el reconocimiento de objetos y la segmentaci\'on de im\'agenes, entre otros.

La visi\'on artificial requiere grandes cantidades de datos para aprender y descubrir patrones. Necesita una exposici\'on extensa a un contenido para adquirir conocimientos sobre \'el. La era de la informaci\'on en la que vivimos actualmente, donde abundan los datos, es el entorno perfecto para que estos algoritmos de aprendizaje se desarrollen. La combinaci\'on de este acceso a conjuntos de datos masivos con las nuevas arquitecturas de aprendizaje profundo ha dado lugar al surgimiento de modelos de visi\'on altamente capacitados. Muchos de los modelos de visi\'on artificial actuales han sido entrenados con cientos de millones de im\'agenes.

Si bien los primeros modelos de visi\'on se especializaban en clasificar objetos espec\'ificos para determinar su presencia en la imagen, con el lanzamiento de la nueva arquitectura de procesamiento del lenguaje, conocida como transformers[\cite{transformers}], en el a\~no 2017, se ha logrado una integraci\'on de las tareas de visi\'on artificial y procesamiento del lenguaje natural, lo cual ha arrojado resultados impresionantes. Un ejemplo de ello es el modelo CLIP[\cite{clip}], entrenado con 400 millones de im\'agenes y texto proveniente de Internet, lo que le permite comprender la similitud existente entre textos e im\'agenes.

\subsection*{Motivaci\'on}
En el contexto de los avances recientes en el campo de la visi\'on artificial, se ha abierto la posibilidad de automatizar el etiquetado de im\'agenes. Los sistemas de recuperaci\'on de informaci\'on m\'as prominentes, como Google, actualmente recuperan im\'agenes utilizando etiquetas asignadas manualmente o palabras clave en la web asociada a la imagen en cuesti\'on. La perspectiva de transferir esta labor manual a m\'aquinas resulta atractiva, aline\'andose con las tendencias actuales de la inteligencia artificial y el aprendizaje autom\'atico, con el potencial de transformar la manera en que gestionamos y organizamos las im\'agenes.

El desarrollo de un sistema que se dedique espec\'ificamente a etiquetar im\'agenes autom\'aticamente y un Sistema de Recuperaci\'on de Im\'agenes para recuperarlas representa un campo poco explorado. El enfoque de hacer esto manual tiene algunas limitaciones. Adem\'as, el proceso de asignaci\'on de etiquetas puede ser laborioso.

\subsection*{Antecedentes}
El campo de la visi\'on artificial ha experimentado un continuo progreso y expansi\'on, dando lugar a diversas arquitecturas y modelos que integran la comprensi\'on de lenguaje y visi\'on. Entre los ejemplos destacados se encuentran CLIP, BLIP[\cite{blip}], LLaVA[\cite{llava}] y GPT-4V[\cite{gpt-4v}].

CLIP (Contrastive Language-Image Pretraining) es un modelo desarrollado y publicado por OpenAI en 2021. Fue concebido con el prop\'osito de comprender y abordar tareas de visi\'on y lenguaje de manera unificada, permitiendo establecer conexiones entre texto e im\'agenes.

BLIP (Bootstrapping Language-Image Pre-training), por su parte, es otro modelo de preentrenamiento de visi\'on y lenguaje desarrollado por Salesforce Research. Hizo su debut en 2022 y, al igual que CLIP, tiene como objetivo comprender y generar tareas de visi\'on y lenguaje de manera conjunta, siendo capaz de generar descripciones precisas de im\'agenes.

LLaVA (Large Language-and-Vision Assistant) es un modelo multimodal de gran escala que combina un codificador de visi\'on con un modelo de lenguaje avanzado para el entendimiento general de contenido visual y lingü\'istico. Fue presentado por un equipo de investigaci\'on de Microsoft en colaboraci\'on con la Universidad de Columbia y la Universidad de Wisconsin-Madison en septiembre de 2023.

GPT-4V, o Modelo de Visi\'on de GPT-4, es una extensi\'on del popular modelo de lenguaje GPT-4 desarrollado por OpenAI. GPT-4V posee la capacidad de comprender im\'agenes y vincularlas con el modelo de lenguaje de GPT-4, lo que permite obtener resultados altamente precisos en tareas relacionadas con visi\'on y lenguaje. Este modelo fue publicado en marzo de 2023, aunque su componente de visi\'on no estuvo disponible para el p\'ublico hasta octubre de 2023.

\subsection*{Problem\'atica}
Aunque estos modelos de visi\'on y lenguaje poseen una notable capacidad para analizar im\'agenes en relaci\'on con el texto, se enfocan en tareas espec\'ificas que difieren de la recuperaci\'on de informaci\'on. A pesar de ello, ofrecen resultados satisfactorios que pueden sentar las bases para abordar de manera efectiva el campo de la recuperaci\'on de im\'agenes.

En la actualidad, los sistemas de recuperaci\'on de im\'agenes se basan en el etiquetado manual, y los sistemas de b\'usqueda no se centran lo suficiente en la recuperaci\'on de im\'agenes en s\'i. En consecuencia, no se dedican a crear un sistema completo de etiquetado y consultas capaz de recuperar im\'agenes desde descripciones detalladas y precisas. En su lugar, se utilizan sistemas de recuperaci\'on de informaci\'on menos precisos para este \'ambito, ya que su objetivo principal suele ser obtener informaci\'on relacionada con las im\'agenes, no las propias im\'agenes.

Ergo, el proceso de etiquetado de im\'agenes exclusivamente con el prop\'osito de recuperarlas con informaci\'on detallada deja margen de mejora. En este trabajo, se busca abordar esta problem\'atica, buscando alcanzar un etiquetado que satisfaga los objetivos planteados y, al mismo tiempo, un procesamiento de las consultas que se ajuste al tipo de etiquetado empleado.
\subsection*{Objetivos}

\subsubsection* {Objetivo general}

El objetivo de este trabajo consiste en desarrollar un sistema automatizado de etiquetado de im\'agenes y un sistema de recuperaci\'on altamente preciso que utilice las etiquetas asignadas, empleando modelos de aprendizaje autom\'atico. El prop\'osito es lograr la recuperaci\'on de la imagen m\'as adecuada mediante consultas que cuenten con descripciones sumamente precisas. Se prestar\'a una atenci\'on especial al formato de las consultas m\'as frecuentemente utilizadas en las b\'usquedas de im\'agenes.

\subsubsection*{Objetivos espec\'ificos}

\begin{itemize}
\item Emplear modelos de aprendizaje autom\'atico entrenados con extensas cantidades de datos especializados en la descripci\'on de im\'agenes.
\item Diseñar una arquitectura escalable que permita la incorporaci\'on de nuevos modelos de visi\'on artificial a medida que este campo se expande con el tiempo.
\item Realizar reentrenamiento de los modelos base utilizados con el fin de mejorar la eficiencia en la descripci\'on de las im\'agenes.
\item Utilizar modelos de segmentaci\'on de im\'agenes para obtener descripciones m\'as detalladas, analizando la imagen no solo en su totalidad, sino tambi\'en por segmentos.
\item Integrar modelos de visi\'on con modelos de lenguaje, como CLIP, para verificar y seleccionar la descripci\'on m\'as precisa de la imagen en cuesti\'on.
\item Procesar las descripciones finales proporcionadas para crear un sistema de tokens que se ajuste al formato de consultas m\'as utilizado, con el objetivo de lograr una recuperaci\'on precisa de la informaci\'on.
\item Desarrollar un sistema \'optimo de recuperaci\'on de informaci\'on para recuperar las im\'agenes almacenadas en la base de datos correspondiente a las im\'agenes procesadas.
\item Realizar un an\'alisis exhaustivo de cada modelo utilizado y plantear soluciones que, por casos de limitaciones de recursos o falta de informaci\'on, su implementaci\'on pr\'actica no es viable.
\end{itemize}


\subsubsection*{Organizaci\'on}
El resto del documento se encuentra organizado de la siguiente manera. En el cap\'itulo 1 se realiza el an\'alisis de una serie de modelos, arquitecturas y trabajos anteriores relacionados con la generaci\'on de texto a partir de im\'agenes. Adem\'as, se exploran t\'ecnicas de recuperaci\'on de informaci\'on con potencial para la extracci\'on de im\'agenes. Este cap\'itulo constituye el estado del arte en el campo.

En el cap\'itulo 2 se lleva a cabo un estudio detallado de cada uno de los modelos empleados para abordar el problema, comparando sus caracter\'isticas y eficiencia.

El cap\'itulo 3 se dedica a explicar la propuesta de soluci\'on, incluyendo la justificaci\'on de la elecci\'on de los modelos y la arquitectura final. Tambi\'en se detallan las propuestas relacionadas con el modelo de recuperaci\'on de informaci\'on y el proceso de reentrenamiento.

En el cap\'itulo 4 se recopilan los detalles de la implementaci\'on y se abordan los desaf\'ios surgidos debido a la limitaci\'on de hardware y acceso a informaci\'on.

El cap\'itulo 5 se enfoca en la comparaci\'on de diversas soluciones, variando los modelos utilizados en cada una de ellas, as\'i como los hiperpar\'ametros modificados. Tambi\'en se evalúa el rendimiento del modelo en su etapa inicial y despu\'es de haber sido reentrenado.

Finalmente, en el cap\'itulo 6 se presentan las conclusiones derivadas de la investigaci\'on llevada a cabo.
