\chapter*{Introducci\'on}\label{chapter:introduction}
\addcontentsline{toc}{chapter}{Introducci\'on}
En el pasado, el uso de la inteligencia artificial estaba restringido y se empleaba principalmente en casos de uso espec\'ificos. Las entidades que la utilizaban sol\'ian estar familiarizadas con este campo y ten\'ian objetivos bien definidos.

En la actualidad se ha logrado un avance considerable en este campo, obteniendo resultados que hace a\~nos parec\'ian poco probables. Cada vez m\'as personas est\'an comenzando a aprovechar estos beneficios, y la tecnolog\'ia est\'a cambiando r\'apidamente, con la inteligencia artificial siendo el centro de todo. Si bien antes esta tecnolog\'ia era menos utilizada,  el lanzamiento de nuevos modelos de lenguaje accesibles para todos, como GPT[\cite{gpt2}], ha despertado el inter\'es y la adopci\'on de la inteligencia artificial por parte de un p\'ublico m\'as amplio.

Es innegable que la interacci\'on entre los seres humanos y las m\'aquinas est\'a experimentando cambios significativos. Cada vez se les encomiendan m\'as tareas que antes eran exclusivas de las personas, como la traducci\'on, el dise\~no de im\'agenes e, incluso, la generaci\'on de c\'odigo, que ahora son abordadas por la inteligencia artificial, al menos hasta cierto grado de correctitud.

Como parte de este avance, el campo de la visi\'on artificial tambi\'en ha evolucionado notablemente. La visi\'on artificial permite a las computadoras y sistemas extraer informaci\'on relevante de im\'agenes digitales, videos y otras entradas visuales. Gracias a esta capacidad, dichos sistemas pueden tomar medidas o realizar recomendaciones basadas en dicha informaci\'on. Se podr\'ia decir que si la inteligencia artificial permite a las computadoras pensar, la visi\'on artificial les permite ver, observar y comprender.

El impresionante progreso del aprendizaje autom\'atico en los \'ultimos a\~nos, especialmente el aprendizaje profundo (\textit{Deep Learning}), ha revolucionado el campo de la visi\'on artificial, posibilitando nuevas aplicaciones que antes parec\'ian inimaginables. Desde diagn\'osticos de im\'agenes en el campo de la medicina, la automatizaci\'on de veh\'iculos, el reconocimiento de objetos y la segmentaci\'on de im\'agenes, entre otros.

La visi\'on artificial requiere grandes cantidades de datos para aprender y descubrir patrones. Necesita una exposici\'on extensa a un contenido para adquirir conocimientos sobre \'el. La era de la informaci\'on en la que se vive actualmente, donde abundan los datos, es el entorno perfecto para que estos algoritmos de aprendizaje se desarrollen. La combinaci\'on de este acceso a conjuntos de datos masivos con las nuevas arquitecturas de aprendizaje profundo ha dado lugar al surgimiento de modelos de visi\'on altamente capacitados. Muchos de los modelos de visi\'on artificial actuales han sido entrenados con cientos de millones de im\'agenes.

Si bien los primeros modelos de visi\'on se especializaban en clasificar objetos espec\'ificos para determinar su presencia en la imagen, con el lanzamiento de la nueva arquitectura de procesamiento del lenguaje, conocida como \textit{transformers}[\cite{transformers}], en el a\~no 2017, se ha logrado una integraci\'on de las tareas de visi\'on artificial y procesamiento del lenguaje natural, lo cual ha arrojado resultados impresionantes. Un ejemplo de ello es el modelo CLIP[\cite{clip-paper}], entrenado con 400 millones de im\'agenes y texto proveniente de Internet, lo que le permite comprender la similitud existente entre textos e im\'agenes.

\section*{Motivaci\'on}
El campo de la recuperación de imágenes a través de características como los \textit{embeddings} ha experimentado un notable avance en los últimos años[\cite{embedding1}][\cite{embedding2}][\cite{Baldrati2022}]. Con la aparición de modelos multimodales que comprenden tanto texto como imágenes dentro del mismo espacio, la creación de sistemas de recuperación que utilizan estas características se presenta como una opción muy útil e innovadora. Estos sistemas de recuperación[\cite{keita2023clip}][\cite{Baldrati2022}][\cite{embedding2}], que emplean técnicas de búsqueda vectorial en lugar de búsqueda basada en palabras clave, buscan grandes colecciones de vectores en un espacio dimensional alto para encontrar vectores similares a una consulta determinada. Este enfoque suele ser más eficaz que las técnicas tradicionales[\cite{Perez2023-Etiquetado}][\cite{RoblesSanchez2004}] de recuperación de imágenes, ya que puede reducir el espacio de búsqueda y mejorar la precisión de los resultados.

Una incrustaci\'on (\textit{embedding}) multimodal es una representación numérica de múltiples tipos de datos, como texto e imágenes, en un espacio común de vectores. Esto permite comparar y combinar información de diferentes modalidades, como el contenido visual y descriptivo textual, para realizar tareas como la búsqueda de imágenes y la clasificación de contenido.

Los \textit{embeddings} multimodales han evolucionado a partir del desarrollo de modelos de procesamiento de lenguaje natural (NLP\supindex{}{Natural Languaje Processing.})[\cite{LLaMA2023}][\cite{gpt3}][\cite{bert}][\cite{gpt4}] y visión por computadora. Inicialmente, los modelos se centraban en una sola modalidad, como texto o imágenes. Con el tiempo, la necesidad de integrar información de diferentes fuentes impulsó el desarrollo de técnicas que pudieran procesar y relacionar datos de múltiples modalidades.


En el campo de la recuperación de imágenes, los \textit{embeddings} multimodales han permitido realizar búsquedas de imágenes basadas en contenido (CBIR\supindex{}{Content-Based Image Retrieval.}) y descripciones textuales, mejorando la relevancia y precisión de los resultados. Los sistemas de CBIR tradicionales dependían de metadatos textuales o características visuales simples, pero los \textit{embeddings} multimodales permiten búsquedas semánticas más complejas que entienden el contenido y el contexto.

El modelo CLIP (\textit{Contrastive Language-Image Pretraining})[\cite{clip-paper}] de \textit{OpenAI} es uno de los avances más significativos en lo que refiere a  \textit{embeddings} multimodales. CLIP combina un modelo de visión por computadora y un modelo de procesamiento de lenguaje natural para aprender representaciones conjuntas de imágenes y texto. CLIP fue desarrollado y publicado por \textit{OpenAI} en el a\~no 2021. Fue concebido con el prop\'osito de comprender y abordar tareas de visi\'on y lenguaje de manera unificada, permitiendo establecer conexiones entre texto e im\'agenes.

Existen otros modelos que abarcan este campo de la comprensión de imágenes y texto de manera conjunta, que se centran más, en la tarea de descripción de imágenes en lenguaje natural. Este tipo de modelos pueden resultar útiles para combinar el uso de \textit{embeddings} y de lenguaje natural, en forma de descripciones, como características de las imágenes. Algunos de los modelos que destacan en este campo son los siguientes:

LLaVA (\textit{Large Language-and-Vision Assistant})[\cite{llava}] es un modelo multimodal de gran escala que combina un codificador de visi\'on con un modelo de lenguaje avanzado para el entendimiento general de contenido visual y lingü\'istico. Fue presentado por un equipo de investigaci\'on de \textit{Microsoft} en colaboraci\'on con la Universidad de Columbia y la Universidad de Wisconsin-Madison en abril del a\~no 2023.

\textit{GPT-4 Vision} (GPT-4V)[\cite{gpt-4v}] es un modelo multimodal de \textit{OpenAI} que puede procesar y comprender tanto texto como imágenes. Entre sus capacidades se encuentra la generación de descripciones de imágenes, lo que le permite elaborar frases descriptivas que pueden ser utilizadas en combinaci\'on con el uso de \textit{embeddings} agregar características en forma de lenguaje natural al contexto de la imagen. Este modelo fue publicado en marzo del a\~no 2023, aunque su componente de visi\'on no estuvo disponible para el p\'ublico hasta octubre del mismo a\~no. Lamentablemente, este modelo no se encuentra disponible para ser utilizado por terceros en versiones gratuitas.

Cada uno de estos modelos ha sido entrenado en tareas específicas que difieren de, precisamente, la recuperación de imágenes a partir de consultas detalladas. Algunas de las funcionalidades más habituales de estos modelos incluyen la descripción de imágenes y la respuesta a preguntas vinculadas con ellas. Sin embargo, el enfoque de la recuperación de imágenes como funcionalidad principal no ha sido el campo más explorado. Por esta razón, se pretende aprovechar las capacidades, ventajas y la extracción de características distintivas de cada uno de estos modelos con el propósito de recuperar, con un alto grado de detalle, imágenes a partir de consultas formuladas en lenguaje natural.

\section*{Problem\'atica}
Aunque los modelos de visión y lenguaje poseen una notable capacidad para analizar imágenes en relación con el texto, actualmente se han enfocado en tareas específicas que difieren de la recuperación precisa de im\'agenes a trav\'es de texto. Sin embargo, han mostrado resultados satisfactorios que pueden sentar las bases para abordar de manera efectiva el campo de la recuperación de imágenes.

Recientemente, el enfoque se ha desplazado hacia la recuperación de imágenes basada en características, con un énfasis particular en el uso de \textit{embeddings}. En el contexto de la recuperación de imágenes, los \textit{embeddings} pueden codificar características visuales y contextuales de las imágenes, lo que las hace adecuadas para tareas de recuperación.

En lugar de centrarse en el etiquetado manual de imágenes, se están dedicando esfuerzos para desarrollar sistemas que utilicen \textit{embeddings} y características para recuperar imágenes. Esta estrategia se centra en la recuperación de las propias imágenes, en lugar de obtener información relacionada con las mismas.

Entonces uno de los retos a enfrentar es el desarrollo sistemas de recuperación de imágenes que pueda generar y utilizar eficazmente los \textit{embeddings} para recuperar imágenes a partir de descripciones detalladas y precisas. Si bien existen trabajos sobre esta tem\'atica, la mayor\'ia de los enfoques para recuperaci\'on de im\'agenes utilizan como entrada de consulta otras im\'agenes, o combinaci\'on de imagen y texto. La tarea de recuperar im\'agenes de manera detallada desde lenguaje natural no ha sido un campo de alto inter\'es.

\section*{Objetivos}

\subsection*{Objetivo general}

El objetivo de este trabajo consiste en proponer un sistema automatizado de extracci\'on de caracter\'isticas de im\'agenes para procesarlas, y un sistema de recuperaci\'on preciso que utilice dichas caracter\'iticas como informaci\'on para crear el proceso de recuperaci\'on, empleando modelos de aprendizaje autom\'atico.


\subsection*{Objetivos espec\'ificos}

\begin{itemize}
\item Realizar un an\'alisis exhaustivo de los trabajos anteriores en este campo, que constituir\'a el cap\'itulo de estado del arte. \ref{chapter:state-of-the-art}   
\item Extraer incrustaciones (\textit{embeddings}) de las im\'agenes utilizando modelos de aprendizaje de m\'aquinas previamente entrenados.
\item Emplear modelos de aprendizaje autom\'atico entrenados con extensas cantidades de datos especializados en la descripci\'on de im\'agenes como una variante de caracter\'isticas de la imagen.
\item Dise\~nar una arquitectura escalable que permita la incorporaci\'on de nuevos modelos de visi\'on artificial a medida que este campo se expande con el tiempo.
\item Utilizar modelos de segmentaci\'on de im\'agenes para obtener descripciones m\'as detalladas, analizando la imagen no solo en su totalidad, sino tambi\'en por segmentos.
\item Procesar las descripciones finales proporcionadas para crear un sistema de tokens que se ajuste al formato de consultas m\'as esperadas, con el objetivo de lograr una recuperaci\'on precisa de la informaci\'on.
\item Desarrollar un prototipo de sistema de recuperaci\'on de informaci\'on para recuperar las im\'agenes almacenadas en la base de datos correspondiente a las im\'agenes procesadas.
\item Validar las propuestas planteadas con resultados experimentales, as\'i como seleccionar, de cada una de ellas, la mejor para el momento actual. Adem\'as, plantear como se comporta cada una de las mismas y como podr\'ian mejorar en futuros trabajos.
\end{itemize}

\section*{Organizaci\'on}
El resto del documento se encuentra organizado de la siguiente manera. En el cap\'itulo 1 se realiza el an\'alisis de una serie de modelos, arquitecturas y trabajos anteriores relacionados con la generaci\'on de texto a partir de im\'agenes. Adem\'as, se exploran t\'ecnicas de recuperaci\'on de informaci\'on con potencial para la extracci\'on de im\'agenes. Este cap\'itulo constituye el estado del arte en el campo.

El cap\'itulo 2 se dedica a explicar la propuesta de soluci\'on, incluyendo la justificaci\'on de la elecci\'on de los modelos y la arquitectura final. Tambi\'en se detallan las propuestas relacionadas con el modelo de recuperaci\'on de informaci\'on.

El cap\'itulo 3 secci\'on 1, se enfoca en la comparaci\'on de diversas soluciones, variando los modelos utilizados en cada una de ellas, as\'i como los hiperpar\'ametros modificados.

En el cap\'itulo 3, seccion 2, se recopilan los detalles de la implementaci\'on y se abordan los desaf\'ios surgidos debido a la limitaci\'on de hardware y acceso a informaci\'on.

Finalmente, en el \'ultimo cap\'itulo se presentan las conclusiones derivadas de la investigaci\'on llevada a cabo.
